DeepSeek (Chinese: 深度求索; pinyin: Shēndù Qiúsuǒ) is a Chinese artificial intelligence company that develops open-source large language models. The company is funded solely by the Chinese hedge fund High-Flyer. Both DeepSeek and High-Flyer are based in Hangzhou, Zhejiang.

Background
High-Flyer was established in 2015 by three engineers from Zhejiang University who started trading during the 2007–2008 financial crisis. The firm used machine learning to trade stocks. In 2019, High-Flyer created High-Flyer AI, a division dedicated to AI research and its applications. By 2021, all of High-Flyer's trading strategies relied on AI, drawing comparisons to the American hedge fund Renaissance Technologies.

In April 2023, High-Flyer announced the creation of an independent body focused on artificial general intelligence (AGI), separate from its financial business. This led to the formation of DeepSeek in May 2023, with funding provided by High-Flyer. Venture capital firms were hesitant to invest, as they felt the company would take too long to provide returns.

Following the release of DeepSeek-V2 in May 2024, which offered strong performance at a low price, DeepSeek ignited a price war in China's AI model market. This led to other major Chinese tech giants such as ByteDance, Tencent, Baidu, and Alibaba reducing their model prices to compete. Despite DeepSeek's low pricing, the company remained profitable while competitors were losing money.

Currently, DeepSeek focuses solely on research and has not yet revealed detailed plans for commercialization. The company targets technical abilities rather than work experience when hiring, leading to a workforce largely composed of recent university graduates or less-established AI developers.

Release History
November 2, 2023: DeepSeek launched its first model, DeepSeek Coder, which was free for both researchers and commercial users, and fully open source under the MIT license.
November 29, 2023: DeepSeek introduced DeepSeek LLM, a 67B parameter model aimed at competing with other large language models (LLMs). It performed similarly to GPT-4 but faced challenges with computational efficiency and scalability. A chatbot version, DeepSeek Chat, was also released.
May 2024: DeepSeek-V2 was launched, with a price of 2 RMB per million output tokens, lower than its competitors, making it a cost-effective option. It ranked seventh on the University of Waterloo Tiger Lab's LLM leaderboard.
November 2024: DeepSeek released DeepSeek R1-Lite-Preview, a model designed for logical inference, mathematical reasoning, and real-time problem-solving. While it exceeded OpenAI’s o1 model on certain benchmarks like the American Invitational Mathematics Examination (AIME) and MATH, it was noted that o1 solved problems faster in some cases.
December 2024: DeepSeek-V3, with 671 billion parameters, was released after 55 days of training at a cost of US$5.58 million. It outperformed models like Llama 3.1 and Qwen 2.5 and matched GPT-4o and Claude 3.5 Sonnet in benchmark tests. This optimization on limited resources highlighted concerns about the potential impact of US sanctions on China's AI development.
January 20, 2025: DeepSeek introduced DeepSeek-R1 and DeepSeek-R1-Zero, both based on the V3-Base model with 671B total parameters and 37B activated parameters. R1-Zero was trained solely using reinforcement learning (RL) with group relative policy optimization (GRPO), producing less readable outputs and switching between English and Chinese in some cases. The company trained R1 to improve these issues and refine its reasoning capabilities. Additionally, "DeepSeek-R1-Distill" models, which are similar to open-weight models like LLaMA and Qwen, were released and fine-tuned on synthetic data generated by R1.
Training Costs of DeepSeek-V3
Pre-training: 2,664 GPU hours, costing US$5.328 million
Context extension: 119 GPU hours, costing US$0.24 million
Fine-tuning: 5 GPU hours, costing US$0.01 million
Total cost: 2,788 GPU hours, costing US$5.576 million
DeepSeek’s optimization techniques allowed for lower resource use compared to its competitors, which is significant given the high costs associated with training large language models.